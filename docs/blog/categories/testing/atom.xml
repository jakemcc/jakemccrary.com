<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: testing | Jake McCrary]]></title>
  <link href="https://jakemccrary.com/blog/categories/testing/atom.xml" rel="self"/>
  <link href="https://jakemccrary.com/"/>
  <updated>2024-02-18T18:20:09-06:00</updated>
  <id>https://jakemccrary.com/</id>
  <author>
    <name><![CDATA[Jake McCrary]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Tests are living documentation]]></title>
    <link href="https://jakemccrary.com/blog/2021/09/11/tests-can-act-as-living-documentation/"/>
    <updated>2021-09-11T18:20:00-05:00</updated>
    <id>https://jakemccrary.com/blog/2021/09/11/tests-can-act-as-living-documentation</id>
    <content type="html"><![CDATA[<p>Tests can serve many purposes.</p>

<p>You might write tests as a way of driving the design of your software.
Other tests might be written in response to a discovered bug and, if written first, those tests you know when you&rsquo;ve fixed the bug and act as guardrails preventing the reintroduction of that bug.
Tests can also be used to confirm you haven&rsquo;t changed behavior while refactoring.</p>

<p>Tests can also be used as documentation.
Unlike non-executable documentation, tests will always match the implementation&rsquo;s behavior.</p>

<p>An example in a comment or other documentation deserves to be in a test.
Take the following sketch of a Clojure function:</p>

<pre><code class="clojure">(defn confobulate
  "Takes a string and transforms it to the confobulated form. Examples:
  - \"alice\" -&gt; \"EcilA\"
  - \"//yolo1\" -&gt; \"//oneOloY\"
  "
  [s]
  (-&gt; s
      ;; insert some work here, not going to implement this
      ))
</code></pre>

<p>The docstring has examples in it to aid humans in understanding its behavior.
These examples are useful!
But they stop being useful and start being dangerous when they stop being accurate.</p>

<p>We can use unit tests to keep examples like this correct.
You can write comments near the assertions letting future readers know about the documentation that needs to be updated if behavior changes.</p>

<pre><code class="clojure">(deftest confobulate-should-ignore-slashes
  ;; If this assertion changes the docstring needs to be updated
  (is (= "//oneOloY" (confobulate "//yolo1"))))

(deftest confobulate-reverses-and-capitalizes
  ;; If this assertion changes the docstring needs to be updated
  (is (= "alice" (confobulate "EcilA"))))
</code></pre>

<p>Any example in a comment or other non-executable documentation should be an assertion in a unit test.
You&rsquo;ve already taken the time to document the behavior; take the time to figure out how to document it in a way that will fail if the behavior changes.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Improve your tests by picking better constants]]></title>
    <link href="https://jakemccrary.com/blog/2021/08/07/improve-your-tests-by-picking-better-constants/"/>
    <updated>2021-08-07T19:58:00-05:00</updated>
    <id>https://jakemccrary.com/blog/2021/08/07/improve-your-tests-by-picking-better-constants</id>
    <content type="html"><![CDATA[<p>The constants you use in unit tests matter.
Like test and variable names, they can improve the readability of your code and make it easier to understand test failures.</p>

<p>Imagine the following.</p>

<p>A new developer joins your team and asks a question about how the code resolves config values.
You are unsure of the details so you pair up with the new teammate to dig into the code.</p>

<p>You know the codebase uses a relatively simple key-value pair concept for configuration.
It reads keys and values from a known files and, based on some rules, either ignores or overrides values when keys are duplicated across files.</p>

<p><code>config-value</code> is the function that looks up the value for a particular configuration key, represented as a <code>string</code>.
This function takes three arguments: an in-memory representation of the configuration files, the key to lookup, and the mode to operate in.
You know the mode is important in influencing how config resolution works but you don&rsquo;t remember the details.</p>

<p>Luckily for you and your pair, the codebase has plenty of unit tests.
The two of you dive in and look at some tests, hoping to understand how config resolution works.</p>

<pre><code class="clojure">(def config {"scratch.conf" {"a" "1"}

             "development.conf" {"a" "2"
                                 "b" "2"}

             "application.conf" {"a" "3"
                                 "b" "3"
                                 "c" "3"}})

(deftest handles-overrides-in-dev-mode
  (is (= "1" (config-value config "a" :dev)))
  (is (= "2" (config-value config "b" :dev)))
  (is (= "3" (config-value config "c" :dev))))

(deftest handles-overrides-in-prod-mode
  (is (= "3" (config-value config "a" :prod)))
  (is (= "3" (config-value config "b" :prod)))
  (is (= "3" (config-value config "c" :prod))))
</code></pre>

<p>It is great that these tests exist but they could be clearer.
They aren&rsquo;t terrible but you have to work a bit understand what is happening.</p>

<p>When reading <code>(= "2" (config-value config "b" :dev))</code>, what does <code>"2"</code> represent?
What does <code>"b"</code> mean?
You have to either keep the value of <code>config</code> in your brain or keep glancing up in the file to recall what it is.</p>

<p>This isn&rsquo;t great.
This adds cognitive overhead that doesn&rsquo;t need to be there.</p>

<p>There are a few ways these tests could be improved
One way is through using better constants.
Let&rsquo;s do a quick rewrite.</p>

<pre><code class="clojure">(def config {"scratch.conf" {"in dev+app+scratch" "from scratch"}

             "development.conf" {"in dev+app+scratch" "from development"
                                 "in dev+app" "from development"}

             "application.conf" {"in dev+app+scratch" "from application"
                                 "in dev+app" "from application"
                                 "in app" "from application"}})

(deftest handles-overrides-in-dev-mode
  (is (= "from scratch" (config-value config "in dev+app+scratch" :dev)))
  (is (= "from development" (config-value config "in dev+app" :dev)))
  (is (= "from application" (config-value config "in app" :dev))))

(deftest handles-overrides-in-prod-mode
  (is (= "from application" (config-value config "in dev+app+scratch" :prod)))
  (is (= "from application" (config-value config "in dev+app" :prod)))
  (is (= "from application" (config-value config "in app" :prod))))
</code></pre>

<p>These are the same tests but with different constants.
Those constants make a huge difference.
This change has made the tests more legible.
You no longer need to remember the value of <code>config</code> or keep glancing up at it to understand the assertions in a test.</p>

<p>You can read <code>(= "from development" (config-value config "in dev+app" :dev))</code> and have a pretty solid idea that you are looking up a key found in both <code>development.conf</code> and <code>application.conf</code> and while in <code>:dev</code> mode expect the value from <code>development.conf</code>.</p>

<p>The new constants provide clues about what the test expects.
You can read and understand the assertions without keeping much state in your head.</p>

<p>This increases the legibility of the tests and is useful when a test fails.
Which of the following is clearer?</p>

<pre><code>FAIL in (handles-overrides-in-dev-mode)
expected: "2"
  actual: "3"
    diff: - "2"
          + "3"
</code></pre>



<pre><code>FAIL in (handles-overrides-in-dev-mode)
expected: "from development"
  actual: "from application"
    diff: - "from development"
          + "from application"
</code></pre>

<p>The second one is clearer.
You can read it and form a hypothesis about what might be broken.</p>

<p>Well chosen constants reduce the state a person needs to keep in their head.
This makes tests easier to understand.
Good constants also make test failures easier to understand.
Just like good variable names, good constants increase the readability of our tests.</p>

<p>It is well worth placing some extra thought into the constants found in your tests.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Bazel to help fix flaky tests]]></title>
    <link href="https://jakemccrary.com/blog/2020/06/28/using-bazel-to-help-fix-flaky-tests/"/>
    <updated>2020-06-28T18:28:00-05:00</updated>
    <id>https://jakemccrary.com/blog/2020/06/28/using-bazel-to-help-fix-flaky-tests</id>
    <content type="html"><![CDATA[<p>Flaky tests are terrible.
These are tests that pass or fail without anything changing in the code.
They often pass the majority of the time and fail rarely.
This makes them hard to detect and cause developers to often just run the tests again.</p>

<p>Flaky tests erode your team&rsquo;s confidence in your system.
They cause folks to get in the habit of not trusting the output of tests.
This discourages people from writing tests as they stop seeing them as something that improves quality and instead view them as a drag on productivity.</p>

<p>Flaky tests are often hard to fix.
If they were easy to fix, they wouldn&rsquo;t have been flaky in the first place.
One difficulty in fixing them is that the failures are often hard to reproduce.</p>

<p>Often, the first step in fixing a flaky test is to write a script to run the tests multiple times in a row.
If you are using <a href="https://bazel.build/">Bazel</a> as your build tool you don&rsquo;t need to write this.</p>

<p>Here is an example <code>bazel</code><sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> command for helping you recreate flaky test failures.</p>

<p><code>bazel test --test_strategy=exclusive --test_output=errors --runs_per_test=50 -t- //...</code></p>

<p>The above command is running all the test targets in a workspace and each flag is important.</p>

<ul>
<li><code>--runs_per_test=50</code> is telling Bazel to run each test 50 times.</li>
<li><code>--test_output=errors</code> is telling Bazel to only print errors to your console.</li>
<li><code>-t-</code> is a shortcut for <code>--nocache_test_results</code> (or <code>--cache_test_results=no</code>).
This flag tells Bazel to <strong>not</strong> cache the test results.</li>
<li><code>--test_strategy=exclusive</code> will cause tests to be run serially.
Without this, Bazel could run your test targets concurrently and if your tests aren&rsquo;t designed for this you may run into other failures.</li>
</ul>


<p>Flaky tests are terrible and you should try not to have them.
Try your best to have reliable tests.</p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
I&rsquo;ve written this while using Bazel 3.2.0. If you are reading this far in the future the flags may have changed.<a href="#fnref:1" rev="footnote">&#8617;</a></li>
</ol>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Breaking change and more in lein-test-refresh 0.24.1]]></title>
    <link href="https://jakemccrary.com/blog/2019/03/20/breaking-change-and-more-in-lein-test-refresh-0-dot-24-dot-0/"/>
    <updated>2019-03-20T21:30:00-05:00</updated>
    <id>https://jakemccrary.com/blog/2019/03/20/breaking-change-and-more-in-lein-test-refresh-0-dot-24-dot-0</id>
    <content type="html"><![CDATA[<p>Today I released <a href="https://github.com/jakemcc/lein-test-refresh">lein-test-refresh</a> <code>0.24.1</code><sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>. I don&rsquo;t always announce new lein-test-refresh versions with an article but this release breaks some existing behavior so I thought it was worth it.</p>

<p>Each of these changes is the direct result of interacting with four different <code>lein-test-refresh</code> users. Some of this took place on GitHub and others through email. Thanks to all of you for taking the time to think about improvements and notice oddities and bring them to my attention.</p>

<h3>Breaking change: Monitoring keystrokes to perform actions</h3>

<p>Prior to this release, if you hit Ctrl-D then STDIN reads an EOF and <code>test-refresh</code> would quit. With version 0.24.1, <code>test-refresh</code> no longer does that. Instead, it stops monitoring for input and just keeps running tests. Since it stops monitoring for input it no longer notices when you hit Enter to cause your tests to rerun. You can still stop <code>lein test-refresh</code> by sending a SIGINT with Ctrl-C.</p>

<p>This change was made because there is some combination of environments where if <code>test-refresh</code> execs <code>/bin/bash</code> then it receives an EOF on STDIN. Before this change, that means <code>test-refresh</code> would quit unexpectedly. Now it will keep going.</p>

<p>Thanks <a href="https://github.com/cloojure">Alan Thompson</a> for bringing this to my attention and taking the time to help diagnose the problem.</p>

<h3>You can supply your own narrowing test selector</h3>

<p>Being able to tell <code>test-refresh</code> to narrow its focus by adding <code>:test-refresh/focus</code> as metadata on a test or namespace has quickly become a favorite feature of many users. Now you can configure a shorter keyword by specifying configuration in your profile. See the <a href="https://github.com/jakemcc/lein-test-refresh/blob/1b5165660d9e40d9394809a95b148ec758a6d56b/sample.project.clj#L61-L65">sample project.clj</a> for how to set this up.</p>

<p>Thanks <a href="https://github.com/metametadata">Yuri Govorushchenko</a> for the suggestion.</p>

<h3>Experimental: Run in a repl</h3>

<p>I&rsquo;ve turned down this feature in the past but a narrower request came up and I thought it seemed useful. <code>test-refresh</code> now exposes a function you can call in a repl to run <code>test-refresh</code> in that repl. This makes the repl useless for any other task. To do this, first add <code>lein-test-refresh</code> as a dependency instead of a plugin to your project.clj. Then, require the namespace and call the function passing in one or more paths to your test directories. Example below.</p>

<pre><code class="clojure">user=&gt; (require 'com.jakemccrary.test-refresh)
nil
user=&gt; (com.jakemccrary.test-refresh/run-in-repl "test")
*********************************************
*************** Running tests ***************
</code></pre>

<p><a href="https://github.com/jakemcc/lein-test-refresh/issues/80">This request</a> was done so that you can run it in Cursive&rsquo;s repl and gain the ability to click on stacktraces. Thanks <a href="https://github.com/klauswuestefeld">Klaus Wuestefeld</a> for bringing this up again with a really solid and focused use case.</p>

<h3>Better output on exceptions while reloading</h3>

<p>This was a <a href="https://github.com/jakemcc/lein-test-refresh/pull/81">pull request</a> from <a href="https://github.com/minhtuannguyen">Minh Tuan Nguyen</a>. Now figuring out where to look for the error will be a little easier.</p>

<h2>Thank you</h2>

<p>Thanks to all the users of lein-test-refresh. I&rsquo;ve found it to be very valuable to the way I work and I&rsquo;m very happy that others do as well.</p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
This was originally 0.24.0 but that had a bug in it. Sorry about that.<a href="#fnref:1" rev="footnote">&#8617;</a></li>
</ol>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Testing asynchronous JavaScript with Jasmine]]></title>
    <link href="https://jakemccrary.com/blog/2019/02/13/testing-asynchronous-javascript-with-jasmine/"/>
    <updated>2019-02-13T17:46:00-06:00</updated>
    <id>https://jakemccrary.com/blog/2019/02/13/testing-asynchronous-javascript-with-jasmine</id>
    <content type="html"><![CDATA[<p>I was recently adding a feature to an internal web UI that caught all unhandled JavaScript errors and reported them to the backend service. The implementation went smoothly with most of the effort spent figuring out how to test the code that was reporting the errors.</p>

<p>If the error reporting failed, I didn&rsquo;t want to trigger reporting another error or completely lose that error. I decided to log a reporting error to the console. I wanted to write a test showing that errors reporting errors were handled so that a future me, or another developer, didn&rsquo;t accidentally remove this special error handling and enable a never ending cycle of of reporting failed reporting attempts.</p>

<p>It took me a while to figure out how to do this. I searched the web and found various articles about using <a href="https://jasmine.github.io/">Jasmine</a> to do async tests. They were helpful but I also wanted to mock out a function, <code>console.error</code>, and assert that it was called. None of the examples I found were explicit about doing something like this. I forget how many different approaches I tried, but it took a while to figure out the below solution.</p>

<p>Here is the code I wanted to test.</p>

<pre><code class="javascript">function reportEvent(event) {
  return fetch('/report-event', {
    method: 'POST',
    headers: {'Content-Type': 'application/json'},
    body: JSON.stringify({name: 'ui name', ...event})
  }).catch(function(e) { console.error('Problem reporting event:', e)});
}
</code></pre>

<p>It takes an incoming <code>event</code> object and merges it with a default value and posts that to the backing service. <code>fetch</code> returns a Promise and the code handles errors by calling <code>catch</code> on it and logging.</p>

<p>Below is what I eventually came up with for testing the error handling feature of <code>reportEvent</code>.</p>

<pre><code class="javascript">describe('reporting events', function() {
  it('logs errors', (done) =&gt; {
    spyOn(console, 'error').and.callFake(() =&gt; {
      expect(console.error).toHaveBeenCalled();
      done();
    });
    spyOn(window, 'fetch').and.returnValue(Promise.reject('error!'));
    reportEvent({level: 'WARN', msg: 'ERROR!'});
  });
});
</code></pre>

<p>This uses <code>spyOn</code> to mock out <code>fetch</code> and <code>console.error</code>. The <code>fetch</code> call is told to return a rejected Promise. The <code>console.error</code> spy is a bit more interesting.</p>

<p>The <code>console.error</code> spy is told to call a fake function. That function asserts that the <code>console.error</code> spy has been called. More importantly, it also calls a <code>done</code> function. That <code>done</code> function is a callback passed to your test by Jasmine. Calling <code>done</code> signals that your async work is completed.</p>

<p>If <code>done</code> is never called then Jasmine will fail the test after some amount of time. By calling <code>done</code> in our <code>console.error</code> fake, we&rsquo;re able to signal to Jasmine that we&rsquo;ve handled the rejected promise.</p>

<p>You don&rsquo;t actually need the <code>expect(console.error).toHaveBeenCalled();</code> as <code>done</code> won&rsquo;t be called unless <code>console.error</code> has been called. If you don&rsquo;t have it though then Jasmine will complain there are no assertions in the test.</p>

<p>So there we have it, an example of using some of Jasmine&rsquo;s asynchronous test features with spies. I wish I had found an article like this when I started this task. Hopefully it saves you, and future me, some time.</p>
]]></content>
  </entry>
  
</feed>
